#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡é‡‘è²»ç‡æ”¶ç›Šè¨ˆç®—æ¨¡çµ„ - æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ç‰ˆæœ¬ v3
å¾æ•¸æ“šåº«è®€å–funding_rate_diffæ•¸æ“šï¼Œè¨ˆç®—å„ç¨®æ™‚é–“é€±æœŸçš„æ”¶ç›ŠæŒ‡æ¨™
è¼¸å‡ºåˆ°æ•¸æ“šåº«: return_metricsè¡¨

=== V3 ä¸»è¦æ”¹é€² ===
1. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ï¼šæª¢æŸ¥æ¯å€‹æ—¥æœŸçš„äº¤æ˜“å°æ•¸é‡æ˜¯å¦å®Œæ•´
2. æ™ºèƒ½å¢é‡è™•ç†ï¼šåªè™•ç†ç¼ºå¤±çš„äº¤æ˜“å°ï¼Œé¿å…é‡è¤‡è¨ˆç®—
3. éŒ¯èª¤éš”é›¢ï¼šå–®å€‹äº¤æ˜“å°å¤±æ•—ä¸å½±éŸ¿å…¶ä»–äº¤æ˜“å°è™•ç†
4. éƒ¨åˆ†æˆåŠŸä¿å­˜ï¼šæˆåŠŸçš„çµæœç«‹å³ä¿å­˜ï¼Œå¤±æ•—çš„è¨˜éŒ„åˆ°æ—¥èªŒ
5. è©³ç´°éŒ¯èª¤è¿½è¹¤ï¼šé€šéæ—¥èªŒæ–‡ä»¶æ¸…æ¥šäº†è§£è™•ç†ç‹€æ…‹
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import argparse
import os
import sys

# æ·»åŠ æ•¸æ“šåº«æ”¯æŒ
from database_operations import DatabaseManager

# ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
os.makedirs("logs", exist_ok=True)

def log_error(message):
    """è¨˜éŒ„éŒ¯èª¤ä¿¡æ¯åˆ°å°ˆç”¨æ—¥èªŒæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file = "logs/calculate_FR_return_list_v3_errors.log"
    
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"{timestamp} - {message}\n")
    
    print(f"âŒ {message}")

def log_info(message):
    """è¨˜éŒ„ä¸€èˆ¬ä¿¡æ¯åˆ°ä¸»æ—¥èªŒæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file = "logs/calculate_FR_return_list_v3.log"
    
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"{timestamp} - {message}\n")
    
    print(f"â„¹ï¸ {message}")

def get_expected_trading_pairs_from_funding_rate_diff(date):
    """
    å¾ funding_rate_diff è¡¨ç²å–æŒ‡å®šæ—¥æœŸæ‡‰è©²å­˜åœ¨çš„äº¤æ˜“å°
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    Returns:
        set: äº¤æ˜“å°é›†åˆ
    """
    try:
        db = DatabaseManager()
        
        query = """
            SELECT DISTINCT symbol || '_' || exchange_a || '_' || exchange_b as trading_pair
            FROM funding_rate_diff 
            WHERE DATE(timestamp_utc) = ?
        """
        
        with db.get_connection() as conn:
            result = pd.read_sql_query(query, conn, params=[date])
        
        if result.empty:
            return set()
        
        return set(result['trading_pair'].tolist())
        
    except Exception as e:
        log_error(f"ç²å– {date} é æœŸäº¤æ˜“å°å¤±æ•—: {e}")
        return set()

def get_existing_trading_pairs_from_return_metrics(date):
    """
    å¾ return_metrics è¡¨ç²å–æŒ‡å®šæ—¥æœŸå¯¦éš›å­˜åœ¨çš„äº¤æ˜“å°
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    Returns:
        set: äº¤æ˜“å°é›†åˆ
    """
    try:
        db = DatabaseManager()
        
        query = """
            SELECT DISTINCT trading_pair
            FROM return_metrics 
            WHERE date = ?
        """
        
        with db.get_connection() as conn:
            result = pd.read_sql_query(query, conn, params=[date])
        
        if result.empty:
            return set()
        
        return set(result['trading_pair'].tolist())
        
    except Exception as e:
        log_error(f"ç²å– {date} ç¾æœ‰äº¤æ˜“å°å¤±æ•—: {e}")
        return set()

def check_data_completeness(date):
    """
    æª¢æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•¸æ“šå®Œæ•´æ€§
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    Returns:
        dict: {
            'date': str,
            'expected_pairs': set,
            'existing_pairs': set,
            'missing_pairs': set,
            'is_complete': bool,
            'expected_count': int,
            'existing_count': int,
            'missing_count': int
        }
    """
    expected_pairs = get_expected_trading_pairs_from_funding_rate_diff(date)
    existing_pairs = get_existing_trading_pairs_from_return_metrics(date)
    missing_pairs = expected_pairs - existing_pairs
    
    return {
        'date': date,
        'expected_pairs': expected_pairs,
        'existing_pairs': existing_pairs,
        'missing_pairs': missing_pairs,
        'is_complete': len(missing_pairs) == 0,
        'expected_count': len(expected_pairs),
        'existing_count': len(existing_pairs),
        'missing_count': len(missing_pairs)
    }

def find_incomplete_dates_and_pairs(start_date, end_date):
    """
    æ‰¾åˆ°éœ€è¦è™•ç†çš„æ—¥æœŸå’Œå°æ‡‰çš„ç¼ºå¤±äº¤æ˜“å°
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
    Returns:
        dict: {
            'date1': ['pair1', 'pair2'],  # éœ€è¦è£œå……çš„äº¤æ˜“å°
            'date2': ['pair3', 'pair4'],  # éœ€è¦è£œå……çš„äº¤æ˜“å°
        }
    """
    print("ğŸ” æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§...")
    
    incomplete_data = {}
    all_dates = generate_date_range(start_date, end_date)
    
    for date in all_dates:
        completeness = check_data_completeness(date)
        
        if not completeness['is_complete']:
            incomplete_data[date] = list(completeness['missing_pairs'])
            print(f"   {date}: {completeness['existing_count']}/{completeness['expected_count']} å€‹äº¤æ˜“å° (ç¼ºå¤± {completeness['missing_count']} å€‹)")
        else:
            print(f"   {date}: âœ… å®Œæ•´ ({completeness['expected_count']} å€‹äº¤æ˜“å°)")
    
    if incomplete_data:
        total_missing = sum(len(pairs) for pairs in incomplete_data.values())
        print(f"ğŸ“Š ç™¼ç¾ {len(incomplete_data)} å€‹ä¸å®Œæ•´æ—¥æœŸï¼Œå…±ç¼ºå¤± {total_missing} å€‹äº¤æ˜“å°æ•¸æ“š")
    else:
        print("âœ… æ‰€æœ‰æ—¥æœŸæ•¸æ“šå®Œæ•´ï¼Œç„¡éœ€è™•ç†")
    
    return incomplete_data

def load_fr_diff_data_from_database(start_date=None, end_date=None, symbol=None):
    """
    å¾æ•¸æ“šåº«åŠ è¼‰æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æ‰€æœ‰FR_diffæ•¸æ“š
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        symbol: äº¤æ˜“å°ç¬¦è™Ÿ (å¯é¸)
    Returns:
        DataFrame with FRå·®ç•°æ•¸æ“š
    """
    try:
        db = DatabaseManager()
        
        print(f"ğŸ“Š æ­£åœ¨å¾æ•¸æ“šåº«åŠ è¼‰FR_diffæ•¸æ“š...")
        if start_date and end_date:
            print(f"   æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
        if symbol:
            print(f"   äº¤æ˜“å°: {symbol}")
        
        # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
        where_conditions = []
        params = []
        
        if start_date:
            where_conditions.append("DATE(timestamp_utc) >= ?")
            params.append(start_date)
            
        if end_date:
            where_conditions.append("DATE(timestamp_utc) <= ?") 
            params.append(end_date)
            
        if symbol:
            where_conditions.append("symbol = ?")
            params.append(symbol)
        
        where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
        
        query = f"""
            SELECT timestamp_utc, symbol, exchange_a, exchange_b, diff_ab,
                   symbol || '_' || exchange_a || '_' || exchange_b as trading_pair
            FROM funding_rate_diff 
            {where_clause}
            ORDER BY timestamp_utc, symbol, exchange_a, exchange_b
        """
        
        with db.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
        
        if df.empty:
            print("âš ï¸ æ•¸æ“šåº«ä¸­æ²’æœ‰æ‰¾åˆ°åŒ¹é…çš„FR_diffæ•¸æ“š")
            return pd.DataFrame()
        
        # è½‰æ›æ™‚é–“æˆ³ä¸¦é‡å‘½ååˆ—ä»¥ä¿æŒå…¼å®¹æ€§
        df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])
        df = df.rename(columns={
            'timestamp_utc': 'Timestamp (UTC)',
            'trading_pair': 'Trading_Pair',
            'diff_ab': 'Diff_AB'
        })
        
        print(f"âœ… æˆåŠŸåŠ è¼‰ {len(df)} è¡ŒFR_diffæ•¸æ“š")
        print(f"   äº¤æ˜“å°æ•¸é‡: {df['Trading_Pair'].nunique()}")
        print(f"   æ™‚é–“ç¯„åœ: {df['Timestamp (UTC)'].min()} åˆ° {df['Timestamp (UTC)'].max()}")
        
        return df
        
    except Exception as e:
        print(f"âŒ å¾æ•¸æ“šåº«åŠ è¼‰FR_diffæ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
        return pd.DataFrame()

def calculate_returns_sql_optimized(start_date, end_date, symbol=None):
    """
    SQLå„ªåŒ–ç‰ˆæœ¬ï¼šä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰äº¤æ˜“å°å’Œæ—¥æœŸçš„æ”¶ç›ŠæŒ‡æ¨™
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        symbol: äº¤æ˜“å°ç¬¦è™Ÿ (å¯é¸)
    Returns:
        DataFrame: åŒ…å«æ‰€æœ‰çµæœçš„DataFrame
    """
    try:
        db = DatabaseManager()
        
        print(f"ğŸš€ SQLå„ªåŒ–ç‰ˆæœ¬ï¼šè¨ˆç®—æ”¶ç›ŠæŒ‡æ¨™...")
        print(f"   æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
        if symbol:
            print(f"   äº¤æ˜“å°: {symbol}")
        
        # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
        where_conditions = ["DATE(timestamp_utc) BETWEEN ? AND ?"]
        params = [start_date, end_date]
        
        if symbol:
            where_conditions.append("symbol = ?")
            params.append(symbol)
        
        where_clause = " AND ".join(where_conditions)
        
        # SQLå„ªåŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨Window Functionsä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰æ”¶ç›ŠæŒ‡æ¨™
        query = f"""
        WITH daily_returns AS (
            -- ç¬¬ä¸€æ­¥ï¼šæŒ‰äº¤æ˜“å°å’Œæ—¥æœŸèšåˆæ¯æ—¥æ”¶ç›Š
            SELECT 
                symbol || '_' || exchange_a || '_' || exchange_b as trading_pair,
                DATE(timestamp_utc) as date,
                SUM(diff_ab) as daily_return
            FROM funding_rate_diff 
            WHERE {where_clause}
            GROUP BY trading_pair, date
            ORDER BY trading_pair, date
        ),
        rolling_calculations AS (
            -- ç¬¬äºŒæ­¥ï¼šä½¿ç”¨Window Functionsè¨ˆç®—æ»‘å‹•çª—å£æ”¶ç›Š
            SELECT 
                trading_pair,
                date,
                daily_return,
                -- 1å¤©æ”¶ç›Š (ç•¶å¤©)
                daily_return as return_1d,
                
                -- 2å¤©æ”¶ç›Š (ç•¶å¤©+å‰1å¤©)
                SUM(daily_return) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
                ) as return_2d,
                
                -- 7å¤©æ”¶ç›Š (ç•¶å¤©+å‰6å¤©)
                SUM(daily_return) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                ) as return_7d,
                
                -- 14å¤©æ”¶ç›Š (ç•¶å¤©+å‰13å¤©)
                SUM(daily_return) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 13 PRECEDING AND CURRENT ROW
                ) as return_14d,
                
                -- 30å¤©æ”¶ç›Š (ç•¶å¤©+å‰29å¤©)
                SUM(daily_return) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
                ) as return_30d,
                
                -- å…¨éƒ¨æ”¶ç›Š (å¾é–‹å§‹åˆ°ç•¶å¤©)
                SUM(daily_return) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS UNBOUNDED PRECEDING
                ) as return_all,
                
                -- è¨ˆç®—å¤©æ•¸ç”¨æ–¼ROIè¨ˆç®—
                COUNT(*) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
                ) as days_2d,
                
                COUNT(*) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                ) as days_7d,
                
                COUNT(*) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 13 PRECEDING AND CURRENT ROW
                ) as days_14d,
                
                COUNT(*) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
                ) as days_30d,
                
                COUNT(*) OVER (
                    PARTITION BY trading_pair 
                    ORDER BY date 
                    ROWS UNBOUNDED PRECEDING
                ) as days_all
                
            FROM daily_returns
        )
        -- ç¬¬ä¸‰æ­¥ï¼šè¨ˆç®—å¹´åŒ–æ”¶ç›Šç‡ä¸¦è¼¸å‡ºæœ€çµ‚çµæœ
        SELECT 
            trading_pair,
            date,
            COALESCE(return_1d, 0.0) as return_1d,
            COALESCE(return_1d * 365, 0.0) as roi_1d,
            
            COALESCE(return_2d, 0.0) as return_2d,
            COALESCE(CASE WHEN days_2d > 0 THEN return_2d * 365.0 / days_2d ELSE 0.0 END, 0.0) as roi_2d,
            
            COALESCE(return_7d, 0.0) as return_7d,
            COALESCE(CASE WHEN days_7d > 0 THEN return_7d * 365.0 / days_7d ELSE 0.0 END, 0.0) as roi_7d,
            
            COALESCE(return_14d, 0.0) as return_14d,
            COALESCE(CASE WHEN days_14d > 0 THEN return_14d * 365.0 / days_14d ELSE 0.0 END, 0.0) as roi_14d,
            
            COALESCE(return_30d, 0.0) as return_30d,
            COALESCE(CASE WHEN days_30d > 0 THEN return_30d * 365.0 / days_30d ELSE 0.0 END, 0.0) as roi_30d,
            
            COALESCE(return_all, 0.0) as return_all,
            COALESCE(CASE WHEN days_all > 0 THEN return_all * 365.0 / days_all ELSE 0.0 END, 0.0) as roi_all
            
        FROM rolling_calculations
        ORDER BY trading_pair, date
        """
        
        print("ğŸ”„ åŸ·è¡ŒSQLæŸ¥è©¢ä¸­...")
        with db.get_connection() as conn:
            results_df = pd.read_sql_query(query, conn, params=params)
        
        if results_df.empty:
            print("âš ï¸ SQLæŸ¥è©¢æ²’æœ‰è¿”å›ä»»ä½•çµæœ")
            return pd.DataFrame()
        
        print(f"âœ… SQLå„ªåŒ–è¨ˆç®—å®Œæˆ!")
        print(f"   ğŸ“Š è¨ˆç®—çµæœ: {len(results_df)} æ¢è¨˜éŒ„")
        print(f"   ğŸ“… æ—¥æœŸç¯„åœ: {results_df['date'].min()} åˆ° {results_df['date'].max()}")
        print(f"   ğŸ”— äº¤æ˜“å°æ•¸é‡: {results_df['trading_pair'].nunique()}")
        
        return results_df
        
    except Exception as e:
        print(f"âŒ SQLå„ªåŒ–è¨ˆç®—æ™‚å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def calculate_returns_for_specific_pairs(date, trading_pairs, start_load_date=None):
    """
    ç‚ºæŒ‡å®šæ—¥æœŸçš„æŒ‡å®šäº¤æ˜“å°è¨ˆç®—æ”¶ç›ŠæŒ‡æ¨™
    Args:
        date: ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)
        trading_pairs: è¦è™•ç†çš„äº¤æ˜“å°åˆ—è¡¨
        start_load_date: æ•¸æ“šåŠ è¼‰é–‹å§‹æ—¥æœŸ (ç”¨æ–¼è¨ˆç®—æ­·å²æ”¶ç›Š)
    Returns:
        DataFrame: è¨ˆç®—çµæœ
    """
    if not trading_pairs:
        return pd.DataFrame()
    
    # å¦‚æœæ²’æœ‰æŒ‡å®šé–‹å§‹æ—¥æœŸï¼Œè‡ªå‹•è¨ˆç®—ï¼ˆéœ€è¦30å¤©æ­·å²æ•¸æ“šï¼‰
    if start_load_date is None:
        start_load_date = (pd.to_datetime(date) - pd.Timedelta(days=30)).strftime('%Y-%m-%d')
    
    print(f"   ğŸ“Š è™•ç† {len(trading_pairs)} å€‹äº¤æ˜“å°: {date}")
    
    # ç‚ºæ¯å€‹äº¤æ˜“å°å–®ç¨è™•ç†ï¼Œå¯¦ç¾éŒ¯èª¤éš”é›¢
    successful_results = []
    failed_pairs = []
    
    for trading_pair in trading_pairs:
        try:
            # æå–äº¤æ˜“å°çš„ symbol ä¿¡æ¯
            symbol = trading_pair.split('_')[0]
            
            # åŠ è¼‰è©²äº¤æ˜“å°çš„æ•¸æ“š
            pair_data = load_fr_diff_data_from_database(
                start_date=start_load_date,
                end_date=date,
                symbol=symbol
            )
            
            if pair_data.empty:
                log_error(f"ç„¡æ³•åŠ è¼‰ {trading_pair} çš„æ•¸æ“š")
                failed_pairs.append(trading_pair)
                continue
            
            # éæ¿¾å‡ºæŒ‡å®šçš„äº¤æ˜“å°
            pair_data = pair_data[pair_data['Trading_Pair'] == trading_pair]
            
            if pair_data.empty:
                log_error(f"{trading_pair} åœ¨ {date} æ²’æœ‰æ•¸æ“š")
                failed_pairs.append(trading_pair)
                continue
            
            # ä½¿ç”¨ç¾æœ‰çš„ SQL å„ªåŒ–è¨ˆç®—æ–¹æ³•
            result = calculate_returns_sql_optimized(start_load_date, date, symbol)
            
            if result.empty:
                log_error(f"{trading_pair} è¨ˆç®—çµæœç‚ºç©º")
                failed_pairs.append(trading_pair)
                continue
            
            # éæ¿¾å‡ºç›®æ¨™æ—¥æœŸå’Œäº¤æ˜“å°çš„çµæœ
            target_result = result[
                (result['date'] == date) & 
                (result['trading_pair'] == trading_pair)
            ]
            
            if target_result.empty:
                log_error(f"{trading_pair} åœ¨ {date} æ²’æœ‰è¨ˆç®—çµæœ")
                failed_pairs.append(trading_pair)
                continue
            
            successful_results.append(target_result)
            print(f"      âœ… {trading_pair}")
            
        except Exception as e:
            log_error(f"{trading_pair} è¨ˆç®—å¤±æ•—: {e}")
            failed_pairs.append(trading_pair)
            continue
    
    # åˆä½µæˆåŠŸçš„çµæœ
    if successful_results:
        final_result = pd.concat(successful_results, ignore_index=True)
        print(f"   âœ… æˆåŠŸè™•ç† {len(successful_results)} å€‹äº¤æ˜“å°")
    else:
        final_result = pd.DataFrame()
        print(f"   âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•äº¤æ˜“å°")
    
    if failed_pairs:
        print(f"   âŒ å¤±æ•— {len(failed_pairs)} å€‹äº¤æ˜“å°")
        log_info(f"{date} å¤±æ•—çš„äº¤æ˜“å°: {failed_pairs}")
    
    return final_result

def process_incomplete_dates_v3(incomplete_data):
    """
    è™•ç†ä¸å®Œæ•´æ—¥æœŸçš„æ•¸æ“šï¼Œæ”¯æŒéŒ¯èª¤éš”é›¢å’Œéƒ¨åˆ†æˆåŠŸä¿å­˜
    Args:
        incomplete_data: {date: [missing_pairs]} æ ¼å¼çš„å­—å…¸
    Returns:
        dict: è™•ç†çµæœçµ±è¨ˆ
    """
    if not incomplete_data:
        print("âœ… æ²’æœ‰éœ€è¦è™•ç†çš„ä¸å®Œæ•´æ•¸æ“š")
        return {'total_dates': 0, 'successful_dates': 0, 'partial_dates': 0, 'failed_dates': 0}
    
    print(f"ğŸ”„ é–‹å§‹è™•ç† {len(incomplete_data)} å€‹ä¸å®Œæ•´æ—¥æœŸ...")
    
    db = DatabaseManager()
    summary = {
        'total_dates': len(incomplete_data),
        'successful_dates': 0,
        'partial_dates': 0,
        'failed_dates': 0,
        'details': {}
    }
    
    for date, missing_pairs in incomplete_data.items():
        print(f"\nğŸ“… è™•ç†æ—¥æœŸ: {date} ({len(missing_pairs)} å€‹ç¼ºå¤±äº¤æ˜“å°)")
        
        # è¨ˆç®—æ•¸æ“šåŠ è¼‰ç¯„åœ
        start_load_date = (pd.to_datetime(date) - pd.Timedelta(days=30)).strftime('%Y-%m-%d')
        
        # è™•ç†è©²æ—¥æœŸçš„ç¼ºå¤±äº¤æ˜“å°
        results_df = calculate_returns_for_specific_pairs(date, missing_pairs, start_load_date)
        
        # çµ±è¨ˆè™•ç†çµæœ
        success_count = len(results_df) if not results_df.empty else 0
        fail_count = len(missing_pairs) - success_count
        
        # ä¿å­˜æˆåŠŸçš„çµæœ
        if not results_df.empty:
            try:
                saved_count = save_to_database_optimized_v3(db, results_df)
                if saved_count > 0:
                    log_info(f"{date} æˆåŠŸä¿å­˜ {saved_count} å€‹äº¤æ˜“å°æ•¸æ“š")
                else:
                    log_error(f"{date} æ•¸æ“šä¿å­˜å¤±æ•—")
            except Exception as e:
                log_error(f"{date} ä¿å­˜æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
        
        # æ›´æ–°çµ±è¨ˆ
        if fail_count == 0:
            summary['successful_dates'] += 1
            summary['details'][date] = f'å®Œå…¨æˆåŠŸ ({success_count}/{len(missing_pairs)})'
        elif success_count > 0:
            summary['partial_dates'] += 1
            summary['details'][date] = f'éƒ¨åˆ†æˆåŠŸ ({success_count}/{len(missing_pairs)})'
        else:
            summary['failed_dates'] += 1
            summary['details'][date] = f'å®Œå…¨å¤±æ•— (0/{len(missing_pairs)})'
        
        print(f"   ğŸ“Š {date} çµæœ: æˆåŠŸ {success_count}, å¤±æ•— {fail_count}")
    
    return summary

def process_batch_data_sql_optimized(start_date, end_date, target_dates, symbol=None):
    """
    SQLå„ªåŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡è™•ç†å¤šå€‹æ—¥æœŸçš„æ•¸æ“š
    Args:
        start_date: æ•¸æ“šé–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: æ•¸æ“šçµæŸæ—¥æœŸ (YYYY-MM-DD)
        target_dates: ç›®æ¨™æ—¥æœŸåˆ—è¡¨
        symbol: äº¤æ˜“å°ç¬¦è™Ÿ (å¯é¸)
    Returns:
        DataFrameåŒ…å«æ‰€æœ‰çµæœ
    """
    print(f"ğŸš€ SQLå„ªåŒ–æ‰¹é‡è™•ç†: {len(target_dates)} å€‹æ—¥æœŸ")
    print(f"   æ•¸æ“šç¯„åœ: {start_date} åˆ° {end_date}")
    
    # ä½¿ç”¨SQLä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰çµæœ
    all_results = calculate_returns_sql_optimized(start_date, end_date, symbol)
    
    if all_results.empty:
        print("âš ï¸ æ²’æœ‰è¨ˆç®—å‡ºä»»ä½•çµæœ")
        return pd.DataFrame()
    
    # éæ¿¾å‡ºç›®æ¨™æ—¥æœŸçš„çµæœ
    target_dates_set = set(target_dates)
    filtered_results = all_results[all_results['date'].isin(target_dates_set)].copy()
    
    if filtered_results.empty:
        print("âš ï¸ ç›®æ¨™æ—¥æœŸæ²’æœ‰åŒ¹é…çš„çµæœ")
        return pd.DataFrame()
    
    print(f"âœ… æ‰¹é‡è™•ç†å®Œæˆ!")
    print(f"   ğŸ“Š ç¸½çµæœ: {len(filtered_results)} æ¢è¨˜éŒ„")
    print(f"   ğŸ“… å¯¦éš›æ—¥æœŸ: {filtered_results['date'].nunique()} å¤©")
    print(f"   ğŸ”— äº¤æ˜“å°: {filtered_results['trading_pair'].nunique()} å€‹")
    
    return filtered_results

def process_daily_data_legacy(combined_df, target_date):
    """
    èˆŠç‰ˆæœ¬çš„è™•ç†å‡½æ•¸ (ä¿ç•™å‘å¾Œå…¼å®¹)
    Args:
        combined_df: åˆä½µçš„FRå·®ç•°æ•¸æ“š
        target_date: ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)
    Returns:
        DataFrameåŒ…å«æ‰€æœ‰äº¤æ˜“å°çš„æ”¶ç›ŠæŒ‡æ¨™
    """
    print(f"âš ï¸ ä½¿ç”¨èˆŠç‰ˆè™•ç†æ–¹å¼è™•ç† {target_date}")
    print("ğŸ’¡ å»ºè­°å‡ç´šåˆ°SQLå„ªåŒ–ç‰ˆæœ¬ä»¥ç²å¾—æ›´å¥½æ€§èƒ½")
    
    # é€™è£¡ä¿ç•™åŸä¾†çš„é‚è¼¯ä½œç‚ºå‚™ç”¨
    # å¯¦éš›ä¸Šæˆ‘å€‘æœƒåœ¨mainå‡½æ•¸ä¸­é¿å…èª¿ç”¨é€™å€‹å‡½æ•¸
    return pd.DataFrame()

def save_returns_to_database(results_df):
    """
    å°‡æ”¶ç›ŠæŒ‡æ¨™ä¿å­˜åˆ°æ•¸æ“šåº«
    Args:
        results_df: åŒ…å«æ”¶ç›ŠæŒ‡æ¨™çš„DataFrame
    Returns:
        ä¿å­˜çš„è¨˜éŒ„æ•¸
    """
    if results_df.empty:
        print("âš ï¸ æ”¶ç›Šæ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•ä¿å­˜")
        return 0
    
    try:
        db = DatabaseManager()
        
        print(f"ğŸ“Š æº–å‚™å°‡ {len(results_df)} æ¢æ”¶ç›ŠæŒ‡æ¨™è¨˜éŒ„æ’å…¥æ•¸æ“šåº«...")
        
        # æº–å‚™æ•¸æ“šåº«æ•¸æ“š
        db_df = results_df.copy()
        
        # è™•ç†åˆ—åæ˜ å°„
        column_mapping = {
            'Trading_Pair': 'trading_pair',
            'Date': 'date',
            '1d_return': 'return_1d',
            '1d_ROI': 'roi_1d',
            '2d_return': 'return_2d', 
            '2d_ROI': 'roi_2d',
            '7d_return': 'return_7d',
            '7d_ROI': 'roi_7d',
            '14d_return': 'return_14d',
            '14d_ROI': 'roi_14d',
            '30d_return': 'return_30d',
            '30d_ROI': 'roi_30d',
            'all_return': 'return_all',
            'all_ROI': 'roi_all'
        }
        
        # é‡å‘½ååˆ—
        for old_col, new_col in column_mapping.items():
            if old_col in db_df.columns:
                db_df[new_col] = db_df[old_col]
        
        # ç¢ºä¿æœ‰æ‰€æœ‰å¿…éœ€çš„åˆ—
        required_columns = ['trading_pair', 'date', 'return_1d', 'roi_1d', 'return_2d', 'roi_2d',
                          'return_7d', 'roi_7d', 'return_14d', 'roi_14d', 'return_30d', 'roi_30d',
                          'return_all', 'roi_all']
        
        for col in required_columns:
            if col not in db_df.columns:
                db_df[col] = 0.0  # é è¨­å€¼
        
        # é¸æ“‡éœ€è¦çš„åˆ—
        db_df = db_df[required_columns].copy()
        
        print(f"ğŸ“Š æ•¸æ“šæ¨£æœ¬: Trading_Pair={db_df.iloc[0]['trading_pair']}, Date={db_df.iloc[0]['date']}")
        
        # ä¿å­˜åˆ°æ•¸æ“šåº«
        inserted_count = db.insert_return_metrics(db_df)
        print(f"âœ… æ•¸æ“šåº«æ’å…¥æˆåŠŸ: {inserted_count} æ¢è¨˜éŒ„")
        
        return inserted_count
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ”¶ç›Šæ•¸æ“šåˆ°æ•¸æ“šåº«æ™‚å‡ºéŒ¯: {e}")
        return 0

def check_existing_return_data():
    """
    æª¢æŸ¥æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ”¶ç›Šæ•¸æ“šï¼Œå›å‚³å·²è™•ç†çš„æ—¥æœŸé›†åˆ
    Returns:
        set: å·²è™•ç†çš„æ—¥æœŸé›†åˆ
    """
    print("ğŸ” æª¢æŸ¥æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ”¶ç›Šæ•¸æ“š...")
    
    try:
        db = DatabaseManager()
        
        # æŸ¥è©¢æ•¸æ“šåº«ä¸­æ‰€æœ‰ä¸é‡è¤‡çš„æ—¥æœŸ
        query = "SELECT DISTINCT date FROM return_metrics ORDER BY date"
        
        with db.get_connection() as conn:
            result = pd.read_sql_query(query, conn)
        
        if result.empty:
            print("ğŸ“Š æ•¸æ“šåº«ä¸­æ²’æœ‰æ”¶ç›Šæ•¸æ“š")
            return set()
        
        existing_dates = set(result['date'].tolist())
        
        print(f"ğŸ“Š æ•¸æ“šåº«ä¸­æ‰¾åˆ° {len(existing_dates)} å€‹å·²è™•ç†çš„æ—¥æœŸ")
        if existing_dates:
            sorted_dates = sorted(existing_dates)
            print(f"ğŸ“… æ•¸æ“šåº«å·²è™•ç†ç¯„åœ: {sorted_dates[0]} åˆ° {sorted_dates[-1]}")
        
        return existing_dates
        
    except Exception as e:
        print(f"âš ï¸ æª¢æŸ¥æ•¸æ“šåº«æ™‚å‡ºéŒ¯: {e}")
        return set()

def auto_detect_date_range():
    """
    è‡ªå‹•æª¢æ¸¬æ•¸æ“šåº«ä¸­funding_rate_diffæ•¸æ“šçš„æ—¥æœŸç¯„åœ
    Returns:
        tuple: (start_date, end_date) æˆ– (None, None)
    """
    print("ğŸ” è‡ªå‹•æƒææ•¸æ“šåº«ä¸­çš„FR_diffæ•¸æ“šç¯„åœ...")
    
    try:
        db = DatabaseManager()
        
        # æŸ¥è©¢æœ€å°å’Œæœ€å¤§æ—¥æœŸ
        query = """
            SELECT MIN(DATE(timestamp_utc)) as min_date, 
                   MAX(DATE(timestamp_utc)) as max_date,
                   COUNT(*) as total_count,
                   COUNT(DISTINCT symbol) as symbol_count
            FROM funding_rate_diff
        """
        
        with db.get_connection() as conn:
            result = conn.execute(query).fetchone()
        
        if not result or result[2] == 0:
            print("âŒ æ•¸æ“šåº«ä¸­æ²’æœ‰æ‰¾åˆ°funding_rate_diffæ•¸æ“š")
            return None, None
        
        min_date = result[0]
        max_date = result[1]
        total_count = result[2]
        symbol_count = result[3]
        
        print(f"ğŸ“ˆ æª¢æ¸¬åˆ°æ•¸æ“šç¯„åœ: {min_date} åˆ° {max_date}")
        print(f"ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {total_count}")
        print(f"ğŸ“… äº¤æ˜“å°æ•¸é‡: {symbol_count}")
        
        return min_date, max_date
        
    except Exception as e:
        print(f"âŒ è‡ªå‹•æª¢æ¸¬æ•¸æ“šç¯„åœæ™‚å‡ºéŒ¯: {e}")
        return None, None

def generate_date_range(start_date, end_date):
    """
    ç”Ÿæˆæ—¥æœŸç¯„åœ
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
    Returns:
        list: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨
    """
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    dates = []
    current_dt = start_dt
    
    while current_dt <= end_dt:
        dates.append(current_dt.strftime('%Y-%m-%d'))
        current_dt += timedelta(days=1)
    
    return dates

def save_to_database_optimized(db, results_df):
    """
    SQLå„ªåŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡ä¿å­˜æ”¶ç›Šæ•¸æ“šåˆ°æ•¸æ“šåº«
    Args:
        db: DatabaseManagerå¯¦ä¾‹
        results_df: åŒ…å«æ”¶ç›Šæ•¸æ“šçš„DataFrame (SQLæŸ¥è©¢çµæœæ ¼å¼)
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    if results_df.empty:
        print("âš ï¸ æ²’æœ‰æ•¸æ“šéœ€è¦ä¿å­˜")
        return False
    
    try:
        print(f"ğŸ’¾ SQLå„ªåŒ–ä¿å­˜: {len(results_df)} æ¢æ”¶ç›Šè¨˜éŒ„...")
        
        # SQLå„ªåŒ–ç‰ˆæœ¬çš„DataFrameå·²ç¶“æœ‰æ­£ç¢ºçš„åˆ—åæ ¼å¼
        # ç¢ºä¿åˆ—åç¬¦åˆæ•¸æ“šåº«æœŸæœ›
        required_columns = [
            'trading_pair', 'date',
            'return_1d', 'roi_1d',
            'return_2d', 'roi_2d', 
            'return_7d', 'roi_7d',
            'return_14d', 'roi_14d',
            'return_30d', 'roi_30d',
            'return_all', 'roi_all'
        ]
        
        # æª¢æŸ¥æ‰€éœ€åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in required_columns if col not in results_df.columns]
        if missing_columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            print(f"   ç¾æœ‰åˆ—: {list(results_df.columns)}")
            return False
        
        # å‰µå»ºç”¨æ–¼æ•¸æ“šåº«æ’å…¥çš„DataFrame
        db_df = results_df[required_columns].copy()
        
        # ç¢ºä¿æ•¸æ“šé¡å‹æ­£ç¢º
        db_df['date'] = pd.to_datetime(db_df['date']).dt.strftime('%Y-%m-%d')
        
        # ç¢ºä¿æ•¸å€¼åˆ—ç‚ºæµ®é»æ•¸
        numeric_columns = [col for col in required_columns if col not in ['trading_pair', 'date']]
        for col in numeric_columns:
            db_df[col] = pd.to_numeric(db_df[col], errors='coerce').fillna(0.0)
        
        print(f"ğŸ“Š æ•¸æ“šç¯„ä¾‹: Trading_Pair={db_df.iloc[0]['trading_pair']}, Date={db_df.iloc[0]['date']}")
        
        # æ‰¹é‡æ’å…¥åˆ°æ•¸æ“šåº«
        inserted_count = db.insert_return_metrics(db_df)
        
        if inserted_count > 0:
            print(f"âœ… SQLå„ªåŒ–ä¿å­˜æˆåŠŸ: {inserted_count} æ¢è¨˜éŒ„")
            return True
        else:
            print("âŒ æ²’æœ‰è¨˜éŒ„è¢«æ’å…¥")
            return False
        
    except Exception as e:
        print(f"âŒ SQLå„ªåŒ–ä¿å­˜æ™‚å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def find_new_dates_to_process(db, start_date, end_date):
    """
    æ‰¾åˆ°éœ€è¦è™•ç†çš„æ–°æ—¥æœŸ
    Args:
        db: DatabaseManagerå¯¦ä¾‹
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
    Returns:
        list: éœ€è¦è™•ç†çš„æ—¥æœŸåˆ—è¡¨
    """
    existing_dates = check_existing_return_data()
    all_dates = generate_date_range(start_date, end_date)
    new_dates = [date for date in all_dates if date not in existing_dates]
    return new_dates

def find_latest_unprocessed_date(db):
    """
    æ‰¾åˆ°æœ€æ–°çš„æœªè™•ç†æ—¥æœŸ
    Args:
        db: DatabaseManagerå¯¦ä¾‹
    Returns:
        str: æœ€æ–°æœªè™•ç†æ—¥æœŸæˆ–None
    """
    # æª¢æŸ¥funding_rate_diffè¡¨ä¸­çš„æœ€æ–°æ—¥æœŸ
    try:
        with db.get_connection() as conn:
            query = "SELECT MAX(DATE(timestamp_utc)) as max_date FROM funding_rate_diff"
            result = conn.execute(query).fetchone()
            
            if not result or not result[0]:
                return None
                
            latest_data_date = result[0]
        
        # æª¢æŸ¥return_metricsè¡¨ä¸­çš„æœ€æ–°æ—¥æœŸ
        existing_dates = check_existing_return_data()
        
        if not existing_dates:
            # å¦‚æœæ²’æœ‰ä»»ä½•è™•ç†éçš„æ—¥æœŸï¼Œè¿”å›æœ€æ–°æ•¸æ“šæ—¥æœŸ
            return latest_data_date
        
        latest_processed_date = max(existing_dates)
        
        # å¦‚æœæœ€æ–°æ•¸æ“šæ—¥æœŸæ¯”æœ€æ–°è™•ç†æ—¥æœŸæ–°ï¼Œè¿”å›æœªè™•ç†çš„æ—¥æœŸ
        if latest_data_date > latest_processed_date:
            return latest_data_date
        
        return None
        
    except Exception as e:
        print(f"âŒ æª¢æŸ¥æœªè™•ç†æ—¥æœŸæ™‚å‡ºéŒ¯: {e}")
        return None

def save_to_database_optimized_v3(db, df):
    """
    V3 ç‰ˆæœ¬çš„æ•¸æ“šåº«ä¿å­˜å‡½æ•¸ï¼Œå¢åŠ éŒ¯èª¤è™•ç†
    Args:
        db: DatabaseManager å¯¦ä¾‹
        df: è¦ä¿å­˜çš„ DataFrame
    Returns:
        int: æˆåŠŸä¿å­˜çš„è¨˜éŒ„æ•¸
    """
    if df.empty:
        return 0
    
    try:
        # ä½¿ç”¨ç¾æœ‰çš„ä¿å­˜å‡½æ•¸
        return save_to_database_optimized(db, df)
    except Exception as e:
        log_error(f"æ•¸æ“šåº«ä¿å­˜å¤±æ•—: {e}")
        return 0

def main():
    print("ğŸš€ è³‡é‡‘è²»ç‡æ”¶ç›Šè¨ˆç®—ç¨‹å¼å•Ÿå‹• (æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ç‰ˆæœ¬ v3)")
    
    parser = argparse.ArgumentParser(description='è¨ˆç®—è³‡é‡‘è²»ç‡æ”¶ç›ŠæŒ‡æ¨™ - æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ç‰ˆæœ¬ v3')
    parser.add_argument('--start-date', type=str, help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='çµæŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--symbol', type=str, help='äº¤æ˜“å°ç¬¦è™Ÿ (å¯é¸)')
    parser.add_argument('--check-only', action='store_true', help='åªæª¢æŸ¥å®Œæ•´æ€§ï¼Œä¸é€²è¡Œè™•ç†')
    parser.add_argument('--process-latest', action='store_true', help='è™•ç†æœ€æ–°çš„æœªè™•ç†æ—¥æœŸ')
    parser.add_argument('--use-legacy', action='store_true', help='ä½¿ç”¨èˆŠç‰ˆè™•ç†æ–¹å¼ (ä¸æ¨è–¦)')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸ“… è³‡é‡‘è²»ç‡æ”¶ç›Šè¨ˆç®— v3 (æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ç‰ˆæœ¬)")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–æ•¸æ“šåº«ç®¡ç†å™¨
        db = DatabaseManager()
        log_info("æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
        
        if args.process_latest:
            # æŸ¥æ‰¾æœ€æ–°çš„æœªè™•ç†æ—¥æœŸ
            latest_date = find_latest_unprocessed_date(db)
            if latest_date:
                log_info(f"ç™¼ç¾æœªè™•ç†æ—¥æœŸ: {latest_date}")
                
                # æª¢æŸ¥è©²æ—¥æœŸçš„å®Œæ•´æ€§
                completeness = check_data_completeness(latest_date)
                if completeness['is_complete']:
                    print(f"âœ… {latest_date} æ•¸æ“šå·²å®Œæ•´ï¼Œç„¡éœ€è™•ç†")
                    return
                
                # è™•ç†ç¼ºå¤±çš„äº¤æ˜“å°
                incomplete_data = {latest_date: list(completeness['missing_pairs'])}
                summary = process_incomplete_dates_v3(incomplete_data)
                
                print(f"\nğŸ“Š è™•ç†å®Œæˆ:")
                print(f"   å®Œå…¨æˆåŠŸ: {summary['successful_dates']} å€‹æ—¥æœŸ")
                print(f"   éƒ¨åˆ†æˆåŠŸ: {summary['partial_dates']} å€‹æ—¥æœŸ")
                print(f"   å®Œå…¨å¤±æ•—: {summary['failed_dates']} å€‹æ—¥æœŸ")
                
            else:
                print("âœ… æ²’æœ‰ç™¼ç¾æœªè™•ç†çš„æ—¥æœŸï¼Œæ‰€æœ‰æ•¸æ“šéƒ½æ˜¯æœ€æ–°çš„")
            return
        
        # å¦‚æœæŒ‡å®šäº†æ—¥æœŸç¯„åœ
        if args.start_date and args.end_date:
            start_date = args.start_date
            end_date = args.end_date
        else:
            # è‡ªå‹•æª¢æ¸¬æ•¸æ“šç¯„åœ
            start_date, end_date = auto_detect_date_range()
            
            if start_date is None or end_date is None:
                print("âŒ ç„¡æ³•æª¢æ¸¬æ•¸æ“šç¯„åœ")
                print("ğŸ’¡ æç¤º:")
                print("   1. æª¢æŸ¥æ•¸æ“šåº«ä¸­æ˜¯å¦æœ‰funding_rate_diffæ•¸æ“š")
                print("   2. æˆ–ä½¿ç”¨ --start-date å’Œ --end-date åƒæ•¸æŒ‡å®šç¯„åœ")
                return
        
        log_info(f"è™•ç†æ—¥æœŸç¯„åœ: {start_date} åˆ° {end_date}")
        
        # V3 æ–°é‚è¼¯ï¼šæŸ¥æ‰¾ä¸å®Œæ•´çš„æ—¥æœŸå’Œç¼ºå¤±çš„äº¤æ˜“å°
        incomplete_data = find_incomplete_dates_and_pairs(start_date, end_date)
        
        if not incomplete_data:
            print("âœ… åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç™¼ç¾éœ€è¦è™•ç†çš„ä¸å®Œæ•´æ•¸æ“š")
            return
        
        # å¦‚æœåªæ˜¯æª¢æŸ¥å®Œæ•´æ€§
        if args.check_only:
            print("\nğŸ“‹ å®Œæ•´æ€§æª¢æŸ¥çµæœ:")
            total_missing = sum(len(pairs) for pairs in incomplete_data.values())
            print(f"   ä¸å®Œæ•´æ—¥æœŸ: {len(incomplete_data)} å€‹")
            print(f"   ç¼ºå¤±äº¤æ˜“å°: {total_missing} å€‹")
            
            for date, missing_pairs in incomplete_data.items():
                print(f"   {date}: ç¼ºå¤± {len(missing_pairs)} å€‹äº¤æ˜“å°")
            
            print("\nğŸ’¡ å¦‚éœ€ä¿®å¾©ï¼Œè«‹ç§»é™¤ --check-only åƒæ•¸é‡æ–°é‹è¡Œ")
            return
        
        # ä½¿ç”¨èˆŠç‰ˆé‚è¼¯ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        if args.use_legacy:
            print("âš ï¸ ä½¿ç”¨èˆŠç‰ˆè™•ç†æ–¹å¼ (æ€§èƒ½è¼ƒä½)")
            print("ğŸ’¡ å»ºè­°ç§»é™¤ --use-legacy åƒæ•¸ä»¥ä½¿ç”¨ v3 æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥")
            
            # è½‰æ›ç‚ºèˆŠç‰ˆæ ¼å¼
            new_dates = list(incomplete_data.keys())
            
            if not new_dates:
                print("âœ… æ²’æœ‰éœ€è¦è™•ç†çš„æ–°æ—¥æœŸ")
                return
            
            # æ“´å±•é–‹å§‹æ—¥æœŸä»¥åŒ…å«è¶³å¤ çš„æ­·å²æ•¸æ“š
            extended_start_date = (pd.to_datetime(start_date) - pd.Timedelta(days=30)).strftime('%Y-%m-%d')
            
            # èˆŠç‰ˆè™•ç†æ–¹å¼
            combined_df = load_fr_diff_data_from_database(extended_start_date, end_date, args.symbol)
            
            if combined_df.empty:
                print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ•¸æ“š")
                return
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(combined_df)} ç­†FRå·®ç•°æ•¸æ“š")
            
            # è™•ç†æ¯å€‹æ–°æ—¥æœŸ
            all_results = []
            for date in new_dates:
                print(f"\nğŸ”„ è™•ç†æ—¥æœŸ: {date}")
                results_df = process_daily_data_legacy(combined_df, date)
                
                if not results_df.empty:
                    all_results.append(results_df)
                    
                    # ä¿å­˜åˆ°æ•¸æ“šåº«
                    success = save_returns_to_database(results_df)
                    if success > 0:
                        print(f"âœ… {date} çš„æ”¶ç›ŠæŒ‡æ¨™å·²ä¿å­˜åˆ°æ•¸æ“šåº« ({success} ç­†è¨˜éŒ„)")
                    else:
                        print(f"âŒ ä¿å­˜ {date} çš„æ•¸æ“šåˆ°æ•¸æ“šåº«å¤±æ•—")
                else:
                    print(f"âš ï¸ {date} æ²’æœ‰è¨ˆç®—å‡ºæ”¶ç›ŠæŒ‡æ¨™")
        else:
            # V3 æ–°é‚è¼¯ï¼šæ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥è™•ç†
            print("ğŸš€ ä½¿ç”¨ v3 æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ç‰ˆæœ¬")
            
            summary = process_incomplete_dates_v3(incomplete_data)
            
            print(f"\nğŸ‰ v3 è™•ç†å®Œæˆ!")
            print(f"   ğŸ“Š è™•ç†çµ±è¨ˆ:")
            print(f"      ç¸½æ—¥æœŸ: {summary['total_dates']}")
            print(f"      å®Œå…¨æˆåŠŸ: {summary['successful_dates']}")
            print(f"      éƒ¨åˆ†æˆåŠŸ: {summary['partial_dates']}")
            print(f"      å®Œå…¨å¤±æ•—: {summary['failed_dates']}")
            
            if summary['details']:
                print(f"   ğŸ“‹ è©³ç´°çµæœ:")
                for date, detail in summary['details'].items():
                    print(f"      {date}: {detail}")
            
            # ç¸½çµ
            if summary['successful_dates'] + summary['partial_dates'] > 0:
                print(f"âœ… æˆåŠŸè™•ç†äº†éƒ¨åˆ†æˆ–å…¨éƒ¨æ•¸æ“š")
            else:
                print(f"âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ•¸æ“šï¼Œè«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶")
            
    except Exception as e:
        log_error(f"ç¨‹å¼åŸ·è¡Œå‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 